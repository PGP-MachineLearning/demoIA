{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCNN-AE Imag ANIMALES.ipynb","provenance":[{"file_id":"1gEcMqfeYOJnjxHE-AACF0t8SJTXIsif6","timestamp":1580735169391}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r6X3sZdb3I7T","colab_type":"text"},"source":["# Demo de Deep Autoencoder (DAE) usando capas ConvNet para procesar las imágenes de algunos TIPOS de ANIMALES\n","Basado en: \n","\n","https://blog.keras.io/building-autoencoders-in-keras.html\n","\n","\n","https://towardsdatascience.com/deep-autoencoders-using-tensorflow-c68f075fd1a3\n","\n","https://medium.com/analytics-vidhya/building-a-convolutional-autoencoder-using-keras-using-conv2dtranspose-ca403c8d144e"]},{"cell_type":"markdown","metadata":{"id":"ZQ3UDqAY4zlD","colab_type":"text"},"source":["1) Importar librerías:"]},{"cell_type":"code","metadata":{"id":"NafdiEwq3IMh","colab_type":"code","colab":{}},"source":["# nota se debe indicar la versión 1 de TF para compatibilidad del código\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","import keras\n","from keras.layers import Input, Dense\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Reshape, Flatten\n","from keras.layers import UpSampling2D, Conv2DTranspose, BatchNormalization\n","from keras.models import Model\n","from keras.utils import plot_model\n","\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","from PIL import Image\n","\n","print(\"\\nLibrerías importadas\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqJT-899FP7H","colab_type":"text"},"source":["2) Definir la configuración del modelo DAE CNN:"]},{"cell_type":"code","metadata":{"id":"i2uoSF7XFQuS","colab_type":"code","colab":{}},"source":["# tamaño de las imágenes\n","# por compatibilidad de tipo de imágenes, para ANIMALES usar sólo en blanco y negro: 1 canal\n","IMAGE_SHAPE = (128, 128, 1)\n","\n","# tamaño de los kernels y pooling (para simplificar son todas iguales)\n","dae_kernel_shape = (3, 3)\n","dae_pooling_shape = (2, 2)\n","\n","# indica la configuración para la parte Encoder \n","#   (cada elemento de las listas son la configuración de las capas Conv)\n","dae_filters = [ 32, 64, 64, 64 ]\n","\n","# la capa de features se define automáticamente \n","# en base a la información de la última capa del Encoder\n","dae_filters.append( 'f' ) \n","dae_posLayFeat = len(dae_filters)-1\n","\n","# indica si el Decoder usa:\n","#  ver 1: Conv2D + UpSampling (mejores colores pero fuera de foco)\n","#  ver 2: Conv2DTranspose + BatchNormalization (mejor nitidez pero como manchada)\n","verCapasDecoder = 2\n","\n","# cantidad de neuronas ocultas para la parte Decoder \n","#   (usa la la lista de Encoder inversa)\n","for eachEncFilter in dae_filters[0:len(dae_filters)-1][::-1]:\n","      dae_filters.append( eachEncFilter )\n","\n","# cantidad de épocas del entrenamiento\n","cantEpocas = 500\n","\n","print(\"Configuración del DAE CNN definida: \")\n","print (\"     -Kernels + Pooling: [\", IMAGE_SHAPE, \"[\", dae_kernel_shape, \"+\", dae_pooling_shape, \"]\", IMAGE_SHAPE, \"] \")\n","print (\"     -Versión Decoder: \", verCapasDecoder,  \" \")\n","print (\"     -Cant. de Filters: [ -,\", dae_filters,  \",- ] \")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pgMOT8WvGWMB","colab_type":"text"},"source":["3) Montar el Drive:"]},{"cell_type":"code","metadata":{"id":"0pkkaUToGKc7","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# directorio local en Google Drive\n","path = 'gdrive/My Drive/IA/demo ANIMALES' \n","imagPath = path + '/imagenes/train' "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMbKRc0E5ONE","colab_type":"text"},"source":["4) Cargar imágenes para entrenar el modelo DAE:"]},{"cell_type":"code","metadata":{"id":"GE_3cMSTyzfG","colab_type":"code","colab":{}},"source":["# cargar las imágenes\n","classes_ori = [] \n","images_ori = []\n","\n","all_dirs = os.listdir( imagPath )\n","for each_dir in all_dirs:\n","\n","    auxiPath = imagPath + '/' + each_dir \n","    imagFN  = os.listdir( auxiPath )\n","    for each_imagFN in imagFN:\n","          \n","          # abre la imagen\n","          imag = Image.open(auxiPath + \"/\" + each_imagFN)\n","          \n","          # ajusta el tamaño\n","          if IMAGE_SHAPE[2]==1:\n","            imag = imag.convert('L')\n","            tipoImage = 'L'\n","          else:\n","            tipoImage = 'RGB'\n","          imag = imag.resize((IMAGE_SHAPE[0], IMAGE_SHAPE[1]), Image.ANTIALIAS)          \n","          \n","          # transforma a un vector de nros\n","          arImag = np.array(imag)\n","          \n","          # agrega a los vectores\n","          classes_ori.append( each_dir )\n","          images_ori.append( arImag )\n","\n","print(\"- Clases cargadas: \", len(classes_ori))\n","print(\"- Imágenes cargadas: \", len(images_ori))\n","\n","if len(images_ori)>0:\n","  print(\"\\n- Ejemplo \", classes_ori[0], \" \", images_ori[0].shape, \": \")\n","  display( Image.fromarray(images_ori[0], tipoImage) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFpF2PNkSbjK","colab_type":"code","colab":{}},"source":["# define función auxiliar para mostrar imágenes preparadas\n","def plot_image(imag):\n","  if IMAGE_SHAPE[2]==1:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1]).astype(np.uint8))\n","    plt.gray()\n","  else:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE).astype(np.uint8))\n","  plt.axis(\"off\")  \n","\n","# define función auxiliar para preparar la lista de imágenes a procesar\n","def prepare_imageList(imagList):    \n","  auxiAr = np.array(imagList).astype('float32') / 255.\n","  auxiAr = auxiAr.reshape((len(auxiAr), IMAGE_SHAPE[0], IMAGE_SHAPE[1], IMAGE_SHAPE[2]))  \n","  return auxiAr\n","\n","# define vector auxiliar para usar en el entrenamiento\n","x_train = prepare_imageList(images_ori)\n","\n","print(\"x_train (cant ejemplos, datos entrada): \", x_train.shape)\n","print(\"\\nImagen reconstruida: \")\n","plot_image(x_train[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOQAmtPAaqZg","colab_type":"text"},"source":["5) Creación del modelo DAE CNN:"]},{"cell_type":"code","metadata":{"id":"LEwtvKsnJLBN","colab_type":"code","colab":{}},"source":["# define la arquitectura de capas del Deep Autoencoder CNN\n","# teniendo en cuenta la definición dada anteriomente\n","input_img_Lay = Input(shape=IMAGE_SHAPE, name='input_img') # capa de entrada\n","eachLay = input_img_Lay\n","auxName = 'enc_'\n","auxId = 1 \n","for i in range(len(dae_filters)):  \n","\n","    # define el nombre de la capa oculta\n","    auxlayerName = auxName+str(auxId)\n","    if i==dae_posLayFeat:\n","        auxlayerName = 'features'\n","        auxName = 'dec_'        \n","    else:        \n","        if auxName == 'enc_':\n","          auxId = auxId + 1\n","        else:\n","          auxId = auxId - 1\n","\n","    # agrega las capas ocultas\n","    if auxlayerName.startswith('enc_'):\n","          # agrega capa Conv2D + MaxPooling para Encoder\n","          eachLay = Conv2D(dae_filters[i], dae_kernel_shape, activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","          eachLay = MaxPooling2D(dae_pooling_shape, padding='same', name='p_'+auxlayerName)(eachLay)\n","\n","    elif auxlayerName.startswith('dec_'):\n","      if verCapasDecoder == 2:\n","          # Dec v2: agrega capa Conv2DTranspose con BatchNormalization para Decoder        \n","          eachLay = Conv2DTranspose(dae_filters[i], dae_kernel_shape, strides=2, activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","          eachLay = BatchNormalization(name='p_'+auxlayerName)(eachLay)\n","      else: \n","          # Dec v1: agrega capa Conv2 + UpSampling2D para Decoder        \n","          eachLay = Conv2D(dae_filters[i], dae_kernel_shape, activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","          eachLay = UpSampling2D(dae_pooling_shape, name='p_'+auxlayerName)(eachLay)\n","\n","    elif auxlayerName.startswith('features'):\n","          #  agrega capa Flatten, Dense y Reshape \n","          # para ello utiliza la información del shape de la última capa Encoder\n","          # y la cantidad de canales de la imagen original\n","          dae_features_shape = (int(eachLay.shape[1]), int(eachLay.shape[2]), IMAGE_SHAPE[2])\n","          num_features = dae_features_shape[0]*dae_features_shape[1]*dae_features_shape[2]      \n","\n","          eachLay = Flatten(name='f_'+auxlayerName)(eachLay)\n","          eachLay = Dense(num_features, activation='sigmoid', name='d_'+auxlayerName)(eachLay)\n","          features_Lay = eachLay\n","          eachLay = Reshape(dae_features_shape, name='r_'+auxlayerName)(eachLay)\n","\n","# agrega la capa de salida usando la cantidad de canales de la imagen como cantidad de filtros   \n","output_img_Lay = Conv2D(IMAGE_SHAPE[2], dae_kernel_shape, activation='sigmoid', padding='same', name='output_img')(eachLay)  # capa de salida\n","\n","# genera el modelo Deep Autoencoder\n","DAEmodel = Model(input_img_Lay, output_img_Lay, name='DAE CNN')\n","DAEmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","\n","print(\"Modelo DAE CNN creado con \", len(DAEmodel.layers), \" capas:\")\n","DAEmodel.summary()\n","print(\"\\n\")\n","plot_model(DAEmodel, show_layer_names=True, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nn7ynSDx0nLt","colab_type":"text"},"source":["5) Entrenar el modelo DAE:"]},{"cell_type":"code","metadata":{"id":"jmBH11GuJhk1","colab_type":"code","colab":{}},"source":["# lleva a cabo el entrenamiento\n","# usando los mismos datos como entrada y salida\n","DAEmodel.fit(x_train, x_train,\n","                epochs = cantEpocas)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RhmI3R31Kzu","colab_type":"text"},"source":["7) Evaluar el modelo DAE entrenado solicitando que reconstruya las imágenes ingresadas:"]},{"cell_type":"code","metadata":{"id":"wfRff6b-LOxG","colab_type":"code","colab":{}},"source":["# evalua al modelo \n","resEval = DAEmodel.evaluate(x_train, x_train)\n","print(\"\\n>Evaluación del Modelo: \")\n","print(\"    - Error: \", resEval[0])\n","print(\"    - Exactitud: \", resEval[1]*100)\n","print(\"\\n\")\n","\n","# procesa las imágenes con el modelo \n","reconstr_imgs = DAEmodel.predict(x_train)\n","\n","# muestra las 15 primeras imágenes \n","print(\"\\n>Resultados: \")\n","for i in range(len(x_train)):\n","\n","    # prepara para mostrar\n","    fig = plt.figure()\n","    fig.suptitle(classes_ori[i])\n","\n","    # muestra la real\n","    ax1 = fig.add_subplot(121)\n","    plot_image(x_train[i])\n","\n","    # muestra la generada por el modelo\n","    ax2 = fig.add_subplot(122)\n","    plot_image(reconstr_imgs[i])\n","\n","    plt.tight_layout()\n","    fig = plt.gcf()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0a79AY0r1Eo","colab_type":"text"},"source":["9) Probar el modelo DAE entrenado con otras imágenes:"]},{"cell_type":"code","metadata":{"id":"jTha04q2Hlsx","colab_type":"code","colab":{}},"source":["# función auxiliar para reconstruir la imagen\n","def reconstruct_image(imag, imagClase):\n","    \n","    # prepara y ajusta el tamaño de la imagen\n","    imagPrep = prepare_imageList([imag])\n","    \n","    # ejecuta el modelo\n","    imagOut = DAEmodel.predict(imagPrep)\n","    \n","    # prepara para mostrar\n","    fig = plt.figure()\n","    fig.suptitle(imagClase)\n","\n","    # muestra la real\n","    ax1 = fig.add_subplot(121)\n","    plot_image(imagPrep)\n","\n","    # muestra la generada por el modelo\n","    ax2 = fig.add_subplot(122)\n","    plot_image(imagOut)\n","\n","    plt.tight_layout()\n","    fig = plt.gcf()\n","\n","\n","# carga las imágenes de prueba\n","imagPathPrueba = path + '/imagenes/test' \n","all_dirs =  os.listdir( imagPathPrueba ) \n","for each_dir in all_dirs:\n","\n","    auxiPath = imagPathPrueba + '/' + each_dir \n","    if os.path.isdir(auxiPath):\n","      imagFN  = os.listdir( auxiPath )\n","      for each_imagFN in imagFN:\n","          \n","          # abre la imagen\n","          imag = Image.open(auxiPath + \"/\" + each_imagFN)\n","          \n","          # ajusta el tamaño\n","          if IMAGE_SHAPE[2]==1:\n","            imag = imag.convert('L')\n","          imag = imag.resize((IMAGE_SHAPE[0], IMAGE_SHAPE[1]), Image.ANTIALIAS)          \n","          \n","          # transforma a un vector de nros\n","          arImag = np.array(imag)\n","\n","          # manda a procesar la imagen cargada\n","          reconstruct_image(arImag, each_imagFN)\n","                  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFl-ipxmHrok","colab_type":"text"},"source":["8) A partir del modelo DAE entrenado, generar dos sub-modelos Encoder y Decoder:"]},{"cell_type":"markdown","metadata":{"id":"eq6z-Gj5fttb","colab_type":"text"},"source":["\n","*   Generar y usar el modelo Encoder para 'clusterizar' las imágenes de entrenamiento:\n"]},{"cell_type":"code","metadata":{"id":"G6aAZfo2H3HW","colab_type":"code","colab":{}},"source":["## Generar el sub-modelo Encoder para Clustering\n","## (desde input hasta features)\n","\n","# reutiliza las capas entrenadas del modelo DAE original\n","clust_input_Lay = input_img_Lay  # capa de entrada\n","clust_output_Lay =  features_Lay  # capa de salida\n","\n","# genera el modelo\n","CLUSTmodel = Model(input_img_Lay, features_Lay, name='Encoder/Clustering')\n","\n","print(\"> Modelo Encoder: \")\n","CLUSTmodel.summary()\n","plot_model(CLUSTmodel, show_layer_names=True, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGA6OetOfuX8","colab_type":"code","colab":{}},"source":["# función auxiliar para generar un gráfico \n","# con los valores codificados \n","# usando PCA para simplificarlos en 2 ejes\n","def genera_grafico_pca(datos, clases, titulo):\n","    pca = PCA(n_components=2)\n","    principalComponents = pca.fit_transform(datos)\n","    principalDf = pd.DataFrame(data = principalComponents,\n","                columns = ['pca_1', 'pca_2'])\n","    finalDf = pd.concat([principalDf, \n","                        pd.DataFrame(clases, columns = ['target'])], \n","                        axis = 1)\n","\n","    fig = plt.figure(figsize = (10,10))\n","    ax = fig.add_subplot(1,1,1) \n","    ax.set_xlabel('PCA1', fontsize = 15)\n","    ax.set_ylabel('PCA2', fontsize = 15)\n","    ax.set_title(titulo, fontsize = 20)\n","    for target in set(classes_ori):\n","        indicesToKeep = finalDf['target'] == target\n","        ax.scatter(finalDf.loc[indicesToKeep, 'pca_2'],\n","                  finalDf.loc[indicesToKeep, 'pca_1'],\n","                  s = 50)\n","    ax.legend(set(classes_ori))\n","    ax.grid()\n","\n","# procesa las imágenes para recibir el valor codificado de cada una\n","x_train_encoded = CLUSTmodel.predict(x_train)\n","\n","# muestra el gráfico con imágenes originales\n","#genera_grafico_pca(x_train, classes_ori, \"Representación de Imágenes Originales\")\n","\n","# codifica los valores codificados como vector\n","x_train_encoded_reshape = []\n","for val in x_train_encoded:\n","  x_train_encoded_reshape.append( val.reshape(num_features) )\n","\n","# muestra estadísticas de los datos codificados\n","minArClust = np.empty(num_features)\n","minArClust.fill(9999.99)\n","maxArClust = np.empty(num_features)\n","maxArClust.fill(-9999.99)\n","sumArClust = np.zeros(num_features)\n","for val in x_train_encoded_reshape:\n","  for i in range(num_features):\n","      sumArClust[i] = sumArClust[i]+val[i]\n","      if val[i]<minArClust[i]: \n","          minArClust[i] = val[i]\n","      if val[i]>maxArClust[i]: \n","          maxArClust[i] = val[i]\n","print(\"\\n\\n> Estadísticas de Clutering de Imágenes codificado en \", num_features,\" valores: \")\n","print(\"- Mínimos:   \", minArClust)\n","print(\"- Máximos:   \", maxArClust)\n","print(\"- Totales:   \", sumArClust)\n","print(\"- Promedios: \", sumArClust/len(x_train_encoded))\n","print(\"\\n\\n\")\n","\n","# muestra el gráfico codificado\n","genera_grafico_pca(x_train_encoded_reshape, classes_ori, \"Representación de Clustering de Imágenes\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_4UqnQtg1hP","colab_type":"text"},"source":["*   Generar y usar el modelo Decoder para generar nuevas imágenes similares a las entrenadas:\n"]},{"cell_type":"code","metadata":{"id":"PHueHIOYOaBg","colab_type":"code","colab":{}},"source":["## Generar el sub-modelo Decoder para Generator\n","## (desde features hasta output)\n","\n","# genera una copia del modelo DAE original para evitar romperlo\n","auxiCloneModel = keras.models.model_from_json(DAEmodel.to_json())\n","#auxiCloneModel.summary()\n","input_gen = Input(shape=(num_features,), name='input_gen') # nueva capa de entrada\n","auxLay_gen = input_gen\n","for pos in range(len(DAEmodel.layers)):\n","\n","    # obtiene el nombre de la capa actual\n","  auxName = DAEmodel.layers[pos].name  \n","  \n","  # sólo considera las capas luego de features (decoder y output)\n","  # para copiar los pesos del DAE original y actualizar la estrcutura\n","  if auxName.startswith('r_features') or auxName.startswith('c_dec_') or auxName.startswith('p_dec_') or auxName=='output_img':   \n","    auxiCloneModel.layers[pos].set_weights(DAEmodel.layers[pos].get_weights()) \n","    auxLay_gen = auxiCloneModel.layers[pos](auxLay_gen) \n","\n","# crea el nuevo modelo Generator\n","GENmodel = Model(input_gen, auxLay_gen, name = 'Decoder/Generator')\n","\n","print(\"> Modelo Decoder: \")\n","GENmodel.summary()\n","plot_model(GENmodel, show_layer_names=True, show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"54A8l5l9St3c","colab_type":"code","colab":{}},"source":["# ejecuta el Generator\n","#  usando valores definidos al azar como datos de entrada\n","\n","cantImagenGenerar = 3\n","consideraEstadClust = True\n","\n","# genera los datos de entrada\n","# (como la codificación tiene varias posiciones con ceros \n","# se considera que sólo se ponen al azar entre 10% y el 70% de los valores, el resto queda en cero,\n","# --sino se podría hacer con \" np.random.rand(cantImagenGenerar, num_features) \"-- )\n","arX = []\n","if consideraEstadClust:\n","    # si están definidas las estadísticas de clustering,\n","    # genera los valores considerandolas\n","    print(\"Genera usando estadísticas de Clustering\")\n","    for i in range(cantImagenGenerar):\n","        X = np.zeros(num_features)\n","        for pos in range(num_features):\n","            if sumArClust[pos]>0:\n","                X[pos] = np.random.uniform(minArClust[pos],maxArClust[pos])\n","        arX.append( X )\n","else:\n","      # si no están definidas las estadísticas de clustering,\n","    # genera los valores totalmente al azar\n","    print(\"Genera usando valores al azar\")\n","    minRnd = num_features*10//100\n","    maxRnd = num_features*70//100\n","    for i in range(cantImagenGenerar):\n","        X = np.zeros(num_features)\n","        for j in range(np.random.randint(low=minRnd, high=maxRnd)):\n","            pos = np.random.randint(low=0, high=num_features-1)\n","            X[pos] = np.random.normal()\n","        arX.append( X )\n","\n","\n","# ejecuta el modelo Generator\n","imagOut = GENmodel.predict( np.array(arX).reshape((len(arX), num_features)) )  \n"," \n","# muestra las imágenes generadas\n","print(\"\\n> Resultados: \")\n","for i in range(len(arX)):\n","\n","    fig = plt.figure()\n","\n","    # muestra los datos (solo primeros 20)\n","    ax1 = fig.add_subplot(121)\n","    datosMostrar = arX[i].reshape(num_features, 1) \n","    ax1.table(cellText=datosMostrar[:20], loc='center')   \n","    ax1.get_xaxis().set_visible(False)\n","    ax1.get_yaxis().set_visible(False)  \n","\n","    #  muestra reconstrucción\n","    ax2 = fig.add_subplot(122)\n","    plot_image(imagOut[i])  \n","\n","    plt.tight_layout()\n","    fig = plt.gcf()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u19qkoTn3U8-","colab_type":"text"},"source":["*   Combinar los resultados de los modelos Clustering y Generator para mostrar que funcionan juntos como el DAE original:"]},{"cell_type":"code","metadata":{"id":"ubf2Yo3J1aEF","colab_type":"code","colab":{}},"source":["# prueba el modelo Generator usando como entrada los datos de Clustering\n","pruebaClust = CLUSTmodel.predict( x_train )\n","pruebaClust_out = GENmodel.predict(  np.array(pruebaClust).reshape((len(pruebaClust), num_features)) )  \n"," \n","# muestra las imágenes generadas\n","print(\"\\n> Resultados (valores de clustering(primeros 20), imagen recounstrida(grande) y original(chica): \")\n","for i in range(len(x_train_encoded)):\n","\n","    fig = plt.figure()\n","\n","    # muestra los datos\n","    ax1 = fig.add_subplot(121)\n","    ax1.table(cellText=pruebaClust[i].reshape(num_features, 1)[:20], loc='center')   \n","    ax1.get_xaxis().set_visible(False)\n","    ax1.get_yaxis().set_visible(False)  \n","\n","    #  muestra reconstrucción\n","    ax2 = fig.add_subplot(122)\n","    plot_image(pruebaClust_out[i])  \n","\n","    # muestra imagen original\n","    ax3 = fig.add_subplot(332)\n","    plot_image(x_train[i])  \n","\n","    fig = plt.gcf()\n"],"execution_count":0,"outputs":[]}]}